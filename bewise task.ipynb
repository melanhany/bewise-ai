{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import pymorphy2\n",
    "import ssl\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yermakhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('punkt')\n",
    "SENT_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_bigrams(text):\n",
    "    double_word = []\n",
    "    for grams in ngrams(text.split(), 2):\n",
    "        double_word.append(grams)\n",
    "    return double_word\n",
    "   # double_word = double_word[:-1]\n",
    "    \n",
    "def return_trigrams(text):\n",
    "    triple_word = []\n",
    "    for grams in ngrams(text.split(), 3):\n",
    "        triple_word.append(grams)\n",
    "    return triple_word\n",
    "   # double_word = double_word[:-1]\n",
    "\n",
    "def return_fograms(text):\n",
    "    quadro_word = []\n",
    "    for grams in ngrams(text.split(), 4):\n",
    "        quadro_word.append(grams)\n",
    "    return quadro_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(df1):\n",
    "    all_bigrams = []\n",
    "    for i in df1.index: # проходимся и добавляем по каждому слову в листе в каждой строке\n",
    "        for j in df1['bigram_list'][i]:\n",
    "            all_bigrams.append(j)\n",
    "    \n",
    "    all_trigrams = []\n",
    "    for i in df1.index:\n",
    "        for j in df1['trigram_list'][i]:\n",
    "            all_trigrams.append(j)\n",
    "\n",
    "    all_fograms = []\n",
    "    for i in df1.index:\n",
    "        for j in df1['fogram_list'][i]:\n",
    "            all_fograms.append(j)\n",
    "\n",
    "    temp_df_bi = pd.DataFrame(all_bigrams).value_counts().head(60).to_frame('count').reset_index()\n",
    "    temp_df_tri = pd.DataFrame(all_trigrams).value_counts().head(30).to_frame('count').reset_index()\n",
    "    temp_df_four = pd.DataFrame(all_fograms).value_counts().head(30).to_frame('count').reset_index()\n",
    "\n",
    "    return temp_df_bi, temp_df_tri, temp_df_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_in_text(text):\n",
    "    prob_thresh = 0.4\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    company_name = ''\n",
    "    for word in word_tokenize(text):\n",
    "        for p in morph.parse(word):\n",
    "            if 'Name' in p.tag and p.score >= prob_thresh:\n",
    "                if occurences(text, company_dict):\n",
    "                    company_name = text.split('компания ', maxsplit=1)[-1].split(' ', 2)[:2]\n",
    "                    company_name = \" \".join(company_name)\n",
    "                    return company_name\n",
    "    return company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_in_text(text):\n",
    "    prob_thresh = 0.4\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    name = ''\n",
    "    for word in word_tokenize(text):\n",
    "        for p in morph.parse(word):\n",
    "            if 'Name' in p.tag and p.score >= prob_thresh:\n",
    "                name = word\n",
    "                return name.title()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurences(text, dictionary):\n",
    "    in_dict = any([word in text for word in dictionary])\n",
    "    if in_dict == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    "    str1 = \"\"\n",
    "    for ele in s:\n",
    "        str1 += ele + ' '\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "greetings = {'здравств', 'добрый'}\n",
    "introduction = {'звать'}\n",
    "company_dict = {'компания'}\n",
    "goodbye = {'хороший вечер', 'до свидание', 'весь добрый'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_data.csv')\n",
    "\n",
    "df['is_greeting'] = ''\n",
    "df['is_introduction'] = ''\n",
    "df['manager_name'] = ''\n",
    "df['company_name'] = ''\n",
    "df['is_goodbye'] = ''\n",
    "\n",
    "df['lemmatized'] = ''\n",
    "\n",
    "\n",
    "#tokenizing and lemmatizing text\n",
    "for i in df.index:\n",
    "    doc2 = Doc(df['text'][i])\n",
    "    doc2.segment(segmenter)\n",
    "    doc2.tag_morph(morph_tagger)\n",
    "    lemmatized_text = []\n",
    "    for token in doc2.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        lemmatized_text.append(token.lemma)\n",
    "    df['lemmatized'][i] = lemmatized_text\n",
    "\n",
    "# parsing name of 'manager' and 'company' from each line of dataset\n",
    "for i in df.index:\n",
    "    if df['role'][i] == 'client' and df['line_n'][i] < 4:\n",
    "        df['manager_name'][i] = name_in_text(df['text'][i])\n",
    "        df['company_name'][i] = company_in_text(df['text'][i])\n",
    "\n",
    "df['lemmatized_string'] = df.apply(lambda x: listToString(x['lemmatized']), axis = 1)\n",
    "df = df[df['lemmatized_string'].isnull() == False]\n",
    "df['bigram_list'] = df.apply(lambda x: return_bigrams(x['lemmatized_string']), axis = 1)\n",
    "df['trigram_list'] = df.apply(lambda x: return_trigrams(x['lemmatized_string']), axis = 1)\n",
    "df['fogram_list'] = df.apply(lambda x: return_fograms(x['lemmatized_string']), axis = 1)\n",
    "\n",
    "df['is_greeting'] = df['lemmatized_string'].apply(lambda x: True if occurences(x, greetings) else False)\n",
    "df['is_introduction'] = df['lemmatized_string'].apply(lambda x: True if occurences(x, introduction) else False)\n",
    "df['is_goodbye'] = df['lemmatized_string'].apply(lambda x: True if occurences(x, goodbye) else False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = df.drop(df.columns[[9,10,11,12,13]], axis = 1, inplace = False)\n",
    "df_prod.to_csv('data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
